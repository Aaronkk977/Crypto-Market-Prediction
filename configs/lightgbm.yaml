# LightGBM Model Configuration

# Data settings
data:
  train_path: "data/train_top200_grouped.parquet"  # Will auto-switch to train_fe_grouped.parquet if cv_method is group_kfold
  test_path: "data/test_top200.parquet"
  target_col: "label"

# Feature selection settings
feature_selection:
  enabled: false  # Set to true to use top N features
  top_n_features: 100  # Number of top features to use
  importance_file: "results/ablation/feature_importance.csv"  # Path to feature importance file

# Split settings
split:
  # For single split (use_cv: false)
  method: "random"  # Options: "random", "time"
  test_size: 0.2
  random_state: 42
  
  # For cross-validation (use_cv: true)
  use_cv: true  # Set to false for single split
  cv_method: "group_kfold"  # Options: "kfold", "group_kfold"
  n_splits: 5  # Number of folds
  group_col: "time_group_id"  # Required for group_kfold
  
  # Note: Use "group_kfold" for time-series data to prevent temporal label leakage
  # Run "python scripts/add_time_groups.py" first to add time_group_id column

# Model settings
model:
  objective: 'regression'
  metric: 'pearson'
  boosting_type: 'gbdt'
  num_leaves: 31
  learning_rate: 0.05
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 5
  max_depth: -1
  min_child_samples: 20
  reg_alpha: 0.0
  reg_lambda: 0.1
  random_state: 42
  n_jobs: 16  # Use multiple CPU cores for parallel training
  verbose: -1
  # Note: GPU training available but currently has CUDA compatibility issues
  # To enable GPU: set device: 'gpu', gpu_platform_id: 0, gpu_device_id: 0
  device: 'cpu'  # Use 'gpu' for GPU training, 'cpu' for CPU training

# Training settings
training:
  num_boost_round: 2500
  early_stopping_rounds: 50
  min_delta: 0.0005

output:
  model_dir: 'artifacts/models'
  model_name: 'lightgbm_model.txt'
